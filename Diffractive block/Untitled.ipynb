{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.testing._internal.common_utils import TestCase\n",
    "from torch.testing._internal.common_utils import dtype2prec_DONTUSE\n",
    "\n",
    "from depthwise_conv3d import DepthwiseConv3d\n",
    "\n",
    "\n",
    "class TestConv(TestCase):\n",
    "    def test_Conv3d_depthwise_naive_groups_cuda(self, dtype=torch.float):\n",
    "        for depth_multiplier in [1, 2]:\n",
    "            m = DepthwiseConv3d(2, 2 * depth_multiplier, kernel_size=3, groups=2).to(\"cuda\", dtype)\n",
    "            i = torch.randn(2, 2, 6, 6, 6, device=\"cuda\", dtype=dtype).div_(2).requires_grad_()\n",
    "            output = m(i)\n",
    "            grad_output = torch.randn(2, 2 * depth_multiplier, 4, 4, 4, device=\"cuda\", dtype=dtype) / 2\n",
    "            output.backward(grad_output)\n",
    "\n",
    "            offset = 1 * depth_multiplier\n",
    "\n",
    "            m1 = DepthwiseConv3d(1, 1 * depth_multiplier, kernel_size=3).to(\"cuda\", dtype)\n",
    "            m1.weight.data = m.weight.data[:offset].clone()\n",
    "            m1.bias.data = m.bias.data[:offset].clone()\n",
    "            i1 = i.detach()[:, :1].clone().requires_grad_()\n",
    "            output1 = m1(i1)\n",
    "            output1.backward(grad_output[:, :offset].contiguous())\n",
    "\n",
    "            m2 = DepthwiseConv3d(1, 1 * depth_multiplier, kernel_size=3).to(\"cuda\", dtype)\n",
    "            m2.weight.data.copy_(m.weight.data[offset:])\n",
    "            m2.bias.data.copy_(m.bias.data[offset:])\n",
    "            i2 = i.detach()[:, 1:].clone().requires_grad_()\n",
    "            output2 = m2(i2)\n",
    "            output2.backward(grad_output[:, offset:].contiguous())\n",
    "\n",
    "            self.assertEqual(output, torch.cat([output1, output2], 1),\n",
    "                             atol=dtype2prec_DONTUSE[dtype], rtol=0)\n",
    "            self.assertEqual(i.grad.data,\n",
    "                             torch.cat([i1.grad.data, i2.grad.data], 1),\n",
    "                             atol=dtype2prec_DONTUSE[dtype], rtol=0)\n",
    "            self.assertEqual(m.bias.grad.data,\n",
    "                             torch.cat([m1.bias.grad.data,\n",
    "                                        m2.bias.grad.data], 0),\n",
    "                             atol=dtype2prec_DONTUSE[dtype], rtol=0)\n",
    "            self.assertEqual(m.weight.grad.data,\n",
    "                             torch.cat([m1.weight.grad.data,\n",
    "                                        m2.weight.grad.data], 0),\n",
    "                             atol=dtype2prec_DONTUSE[dtype], rtol=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = TestConv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.grad_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from depthwise_conv3d import DepthwiseConv3d\n",
    "\n",
    "dtype = torch.float\n",
    "# conv = DepthwiseConv3d(64, 64, kernel_size=7, groups=1, padding = 3).to(\"cuda\", dtype)\n",
    "# input = torch.randn(2, 64, 128, 128, 128, device=\"cuda\", dtype=dtype).div_(2).requires_grad_()\n",
    "# output = conv(input)\n",
    "# print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import math\n",
    "\n",
    "class diff_net(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(diff_net, self).__init__()\n",
    "#         self.A = nn.Parameter(torch.rand((1,1,6,6,6)))\n",
    "        self.weight = nn.Parameter(torch.Tensor(1,input_channels,128,128,128).cuda())\n",
    "        self.diff = DepthwiseConv3d(input_channels, input_channels, kernel_size=3, groups=1, padding = 1).to(\"cuda\", dtype)\n",
    "        self.reset_parameters()\n",
    "    def reset_parameters(self):\n",
    "        n = 1\n",
    "        for k in (3,3,3):\n",
    "            n *= k\n",
    "        stdv = 1. / math.sqrt(n)\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x*(1/self.weight)\n",
    "        \n",
    "        x = self.diff(x)\n",
    "        #print(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5, 5)\n",
      "torch.Size([5, 5, 5])\n",
      "torch.Size([1, 1, 5, 5, 5])\n",
      "torch.Size([1, 1, 5, 5, 5])\n",
      "torch.Size([1, 1, 5, 5, 5])\n",
      "tensor([[[[[1., 1., 1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1., 1., 1.],\n",
      "           [1., 1., 1., 1., 1., 1.]]]]], device='cuda:0')\n",
      "tensor([[[[[ -33.5053,  -44.6612,  -55.8175,  -55.8175,  -44.6612,  -33.5053],\n",
      "           [ -44.6612,  -59.5310,  -74.4014,  -74.4014,  -59.5310,  -44.6612],\n",
      "           [ -55.8175,  -74.4014,  -92.9861,  -92.9861,  -74.4014,  -55.8175],\n",
      "           [ -55.8175,  -74.4014,  -92.9861,  -92.9861,  -74.4014,  -55.8175],\n",
      "           [ -44.6612,  -59.5310,  -74.4014,  -74.4014,  -59.5310,  -44.6612],\n",
      "           [ -33.5053,  -44.6612,  -55.8175,  -55.8175,  -44.6612,  -33.5053]],\n",
      "\n",
      "          [[ -44.6612,  -59.5310,  -74.4014,  -74.4014,  -59.5310,  -44.6612],\n",
      "           [ -59.5310,  -79.3510,  -99.1718,  -99.1718,  -79.3510,  -59.5310],\n",
      "           [ -74.4014,  -99.1718, -123.9434, -123.9434,  -99.1718,  -74.4014],\n",
      "           [ -74.4014,  -99.1718, -123.9434, -123.9434,  -99.1718,  -74.4014],\n",
      "           [ -59.5310,  -79.3510,  -99.1718,  -99.1718,  -79.3510,  -59.5310],\n",
      "           [ -44.6611,  -59.5310,  -74.4014,  -74.4014,  -59.5310,  -44.6611]],\n",
      "\n",
      "          [[ -55.8175,  -74.4014,  -92.9861,  -92.9861,  -74.4014,  -55.8175],\n",
      "           [ -74.4014,  -99.1718, -123.9434, -123.9434,  -99.1718,  -74.4014],\n",
      "           [ -92.9861, -123.9434, -154.9019, -154.9019, -123.9434,  -92.9861],\n",
      "           [ -92.9861, -123.9434, -154.9019, -154.9019, -123.9434,  -92.9861],\n",
      "           [ -74.4014,  -99.1719, -123.9434, -123.9434,  -99.1718,  -74.4014],\n",
      "           [ -55.8175,  -74.4014,  -92.9861,  -92.9861,  -74.4014,  -55.8175]],\n",
      "\n",
      "          [[ -55.8175,  -74.4014,  -92.9862,  -92.9862,  -74.4014,  -55.8175],\n",
      "           [ -74.4014,  -99.1718, -123.9434, -123.9434,  -99.1719,  -74.4014],\n",
      "           [ -92.9862, -123.9434, -154.9019, -154.9019, -123.9434,  -92.9861],\n",
      "           [ -92.9861, -123.9434, -154.9020, -154.9020, -123.9434,  -92.9861],\n",
      "           [ -74.4014,  -99.1718, -123.9434, -123.9434,  -99.1718,  -74.4014],\n",
      "           [ -55.8175,  -74.4014,  -92.9861,  -92.9861,  -74.4014,  -55.8175]],\n",
      "\n",
      "          [[ -44.6611,  -59.5310,  -74.4014,  -74.4014,  -59.5310,  -44.6611],\n",
      "           [ -59.5310,  -79.3510,  -99.1718,  -99.1718,  -79.3510,  -59.5310],\n",
      "           [ -74.4014,  -99.1718, -123.9433, -123.9433,  -99.1718,  -74.4014],\n",
      "           [ -74.4014,  -99.1718, -123.9434, -123.9434,  -99.1718,  -74.4014],\n",
      "           [ -59.5310,  -79.3510,  -99.1718,  -99.1718,  -79.3510,  -59.5310],\n",
      "           [ -44.6611,  -59.5310,  -74.4014,  -74.4014,  -59.5310,  -44.6611]],\n",
      "\n",
      "          [[ -33.5053,  -44.6611,  -55.8175,  -55.8175,  -44.6611,  -33.5053],\n",
      "           [ -44.6611,  -59.5310,  -74.4014,  -74.4014,  -59.5310,  -44.6611],\n",
      "           [ -55.8175,  -74.4014,  -92.9861,  -92.9861,  -74.4014,  -55.8175],\n",
      "           [ -55.8175,  -74.4014,  -92.9861,  -92.9861,  -74.4014,  -55.8175],\n",
      "           [ -44.6611,  -59.5310,  -74.4014,  -74.4014,  -59.5310,  -44.6611],\n",
      "           [ -33.5053,  -44.6611,  -55.8175,  -55.8175,  -44.6611,  -33.5053]]]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[[[0.5230, 0.5316, 0.5317, 0.5317, 0.5316, 0.5230],\n",
      "           [0.5316, 0.5413, 0.5414, 0.5414, 0.5413, 0.5316],\n",
      "           [0.5317, 0.5414, 0.5415, 0.5415, 0.5414, 0.5317],\n",
      "           [0.5317, 0.5414, 0.5415, 0.5415, 0.5414, 0.5317],\n",
      "           [0.5316, 0.5413, 0.5414, 0.5414, 0.5413, 0.5316],\n",
      "           [0.5230, 0.5316, 0.5317, 0.5317, 0.5316, 0.5230]],\n",
      "\n",
      "          [[0.5316, 0.5413, 0.5414, 0.5414, 0.5413, 0.5316],\n",
      "           [0.5413, 0.5521, 0.5522, 0.5522, 0.5521, 0.5413],\n",
      "           [0.5414, 0.5522, 0.5523, 0.5523, 0.5522, 0.5414],\n",
      "           [0.5414, 0.5522, 0.5523, 0.5523, 0.5522, 0.5414],\n",
      "           [0.5413, 0.5521, 0.5522, 0.5522, 0.5521, 0.5413],\n",
      "           [0.5316, 0.5413, 0.5414, 0.5414, 0.5413, 0.5316]],\n",
      "\n",
      "          [[0.5317, 0.5414, 0.5415, 0.5415, 0.5414, 0.5317],\n",
      "           [0.5414, 0.5522, 0.5523, 0.5523, 0.5522, 0.5414],\n",
      "           [0.5415, 0.5523, 0.5525, 0.5525, 0.5523, 0.5415],\n",
      "           [0.5415, 0.5523, 0.5525, 0.5525, 0.5523, 0.5415],\n",
      "           [0.5414, 0.5522, 0.5523, 0.5523, 0.5522, 0.5414],\n",
      "           [0.5317, 0.5414, 0.5415, 0.5415, 0.5414, 0.5317]],\n",
      "\n",
      "          [[0.5317, 0.5414, 0.5415, 0.5415, 0.5414, 0.5317],\n",
      "           [0.5414, 0.5522, 0.5523, 0.5523, 0.5522, 0.5414],\n",
      "           [0.5415, 0.5523, 0.5525, 0.5525, 0.5523, 0.5415],\n",
      "           [0.5415, 0.5523, 0.5525, 0.5525, 0.5523, 0.5415],\n",
      "           [0.5414, 0.5522, 0.5523, 0.5523, 0.5522, 0.5414],\n",
      "           [0.5317, 0.5414, 0.5415, 0.5415, 0.5414, 0.5317]],\n",
      "\n",
      "          [[0.5316, 0.5413, 0.5414, 0.5414, 0.5413, 0.5316],\n",
      "           [0.5413, 0.5521, 0.5522, 0.5522, 0.5521, 0.5413],\n",
      "           [0.5414, 0.5522, 0.5523, 0.5523, 0.5522, 0.5414],\n",
      "           [0.5414, 0.5522, 0.5523, 0.5523, 0.5522, 0.5414],\n",
      "           [0.5413, 0.5521, 0.5522, 0.5522, 0.5521, 0.5413],\n",
      "           [0.5316, 0.5413, 0.5414, 0.5414, 0.5413, 0.5316]],\n",
      "\n",
      "          [[0.5230, 0.5316, 0.5317, 0.5317, 0.5316, 0.5230],\n",
      "           [0.5316, 0.5413, 0.5414, 0.5414, 0.5413, 0.5316],\n",
      "           [0.5317, 0.5414, 0.5415, 0.5415, 0.5414, 0.5317],\n",
      "           [0.5317, 0.5414, 0.5415, 0.5415, 0.5414, 0.5317],\n",
      "           [0.5316, 0.5413, 0.5414, 0.5414, 0.5413, 0.5316],\n",
      "           [0.5230, 0.5316, 0.5317, 0.5317, 0.5316, 0.5230]]]]],\n",
      "       device='cuda:0', grad_fn=<DepthwiseConv3dFunctionBackward>)\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "input = torch.ones(1, 1, 6, 6, 6, device=\"cuda\", dtype=dtype).requires_grad_()\n",
    "input = input.type(torch.cuda.FloatTensor)\n",
    "diff = DepthwiseConv3d(1, 1, kernel_size=5, groups=1, padding = 2).to(\"cuda\", dtype)\n",
    "y = diff(input)\n",
    "loss = (y-y+1).sum()\n",
    "loss.backward()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 3)\n",
      "torch.Size([3, 3, 3])\n",
      "tensor([[[[[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           ...,\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.]],\n",
      "\n",
      "          [[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           ...,\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.]],\n",
      "\n",
      "          [[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           ...,\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           ...,\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.]],\n",
      "\n",
      "          [[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           ...,\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.]],\n",
      "\n",
      "          [[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           ...,\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.]]],\n",
      "\n",
      "\n",
      "         [[[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           ...,\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.]],\n",
      "\n",
      "          [[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           ...,\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.]],\n",
      "\n",
      "          [[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           ...,\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           ...,\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.]],\n",
      "\n",
      "          [[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           ...,\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.]],\n",
      "\n",
      "          [[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           ...,\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.]]],\n",
      "\n",
      "\n",
      "         [[[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           ...,\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.]],\n",
      "\n",
      "          [[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           ...,\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.]],\n",
      "\n",
      "          [[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           ...,\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           ...,\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.]],\n",
      "\n",
      "          [[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           ...,\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.]],\n",
      "\n",
      "          [[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           ...,\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           ...,\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.]],\n",
      "\n",
      "          [[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           ...,\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.]],\n",
      "\n",
      "          [[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           ...,\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           ...,\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.]],\n",
      "\n",
      "          [[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           ...,\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.]],\n",
      "\n",
      "          [[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           ...,\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.]]],\n",
      "\n",
      "\n",
      "         [[[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           ...,\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.]],\n",
      "\n",
      "          [[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           ...,\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.]],\n",
      "\n",
      "          [[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           ...,\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           ...,\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.]],\n",
      "\n",
      "          [[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           ...,\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.]],\n",
      "\n",
      "          [[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           ...,\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.]]],\n",
      "\n",
      "\n",
      "         [[[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           ...,\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.]],\n",
      "\n",
      "          [[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           ...,\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.]],\n",
      "\n",
      "          [[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           ...,\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           ...,\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.]],\n",
      "\n",
      "          [[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           ...,\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.]],\n",
      "\n",
      "          [[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           ...,\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "           [2., 2., 2.,  ..., 2., 2., 2.]]]]], device='cuda:0')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Depthwise weight should have in_channels=1, got 64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-5b3e1e2eb631>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# 前向传播\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m99999\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-4d0593ee234f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;31m#print(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/houqingfan/Pytorch-Depthwise-Conv3d-master/depthwise_conv3d.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         return depthwise_conv3d(x, self.R_, self.bias, self.stride, self.padding, self.dilation,\n\u001b[0;32m--> 108\u001b[0;31m                                 self.groups, )\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/cuda/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_fwd\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcast_inputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fwd_used_autocast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_autocast_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0mautocast_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_autocast_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/houqingfan/Pytorch-Depthwise-Conv3d-master/depthwise_conv3d.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, input, weight, bias, stride, padding, dilation, groups)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             ctx.dilation)\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_for_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Depthwise weight should have in_channels=1, got 64"
     ]
    }
   ],
   "source": [
    "# 定义模型\n",
    "model = diff_net(64)\n",
    "# 将模型中所有参数拷贝到GPU端\n",
    "# 定义优化器\n",
    "input = torch.ones(1, 64, 128, 128, 128, device=\"cuda\", dtype=dtype)\n",
    "target = torch.ones(1, 64, 128, 128, 128, device=\"cuda\", dtype=dtype)\n",
    "target = target*2\n",
    "# print(target[0,0,:,2,2].shape)\n",
    "# target[0,0,:,2:4,2:4] = (torch.ones(6,2,2)*2).cuda()\n",
    "\n",
    "# target = torch.tensor([[[[[2.8893, 3.6438, 3.6557, 3.6557, 3.6438, 2.8893],\n",
    "#            [3.6438, 4.5361, 4.5508, 4.5508, 4.5361, 3.6438],\n",
    "#            [3.6557, 4.5508, 4.5656, 4.5656, 4.5508, 3.6557],\n",
    "#            [3.6557, 4.5508, 4.5656, 4.5656, 4.5508, 3.6557],\n",
    "#            [3.6438, 4.5361, 4.5508, 4.5508, 4.5361, 3.6438],\n",
    "#            [2.8893, 3.6438, 3.6557, 3.6557, 3.6438, 2.8893]],\n",
    "\n",
    "#           [[3.6438, 4.5361, 4.5508, 4.5508, 4.5361, 3.6438],\n",
    "#            [4.5361, 5.5912, 5.6094, 5.6094, 5.5912, 4.5361],\n",
    "#            [4.5508, 5.6094, 5.6278, 5.6278, 5.6094, 4.5508],\n",
    "#            [4.5508, 5.6094, 5.6278, 5.6278, 5.6094, 4.5508],\n",
    "#            [4.5361, 5.5912, 5.6094, 5.6094, 5.5912, 4.5361],\n",
    "#            [3.6438, 4.5361, 4.5508, 4.5508, 4.5361, 3.6438]],\n",
    "\n",
    "#           [[3.6557, 4.5508, 4.5656, 4.5656, 4.5508, 3.6557],\n",
    "#            [4.5508, 5.6094, 5.6278, 5.6278, 5.6094, 4.5508],\n",
    "#            [4.5656, 5.6278, 5.6465, 5.6465, 5.6278, 4.5656],\n",
    "#            [4.5656, 5.6278, 5.6465, 5.6465, 5.6278, 4.5656],\n",
    "#            [4.5508, 5.6094, 5.6278, 5.6278, 5.6094, 4.5508],\n",
    "#            [3.6557, 4.5508, 4.5656, 4.5656, 4.5508, 3.6557]],\n",
    "\n",
    "#           [[3.6557, 4.5508, 4.5656, 4.5656, 4.5508, 3.6557],\n",
    "#            [4.5508, 5.6094, 5.6278, 5.6278, 5.6094, 4.5508],\n",
    "#            [4.5656, 5.6278, 5.6465, 5.6465, 5.6278, 4.5656],\n",
    "#            [4.5656, 5.6278, 5.6465, 5.6465, 5.6278, 4.5656],\n",
    "#            [4.5508, 5.6094, 5.6278, 5.6278, 5.6094, 4.5508],\n",
    "#            [3.6557, 4.5508, 4.5656, 4.5656, 4.5508, 3.6557]],\n",
    "\n",
    "#           [[3.6438, 4.5361, 4.5508, 4.5508, 4.5361, 3.6438],\n",
    "#            [4.5361, 5.5912, 5.6094, 5.6094, 5.5912, 4.5361],\n",
    "#            [4.5508, 5.6094, 5.6278, 5.6278, 5.6094, 4.5508],\n",
    "#            [4.5508, 5.6094, 5.6278, 5.6278, 5.6094, 4.5508],\n",
    "#            [4.5361, 5.5912, 5.6094, 5.6094, 5.5912, 4.5361],\n",
    "#            [3.6438, 4.5361, 4.5508, 4.5508, 4.5361, 3.6438]],\n",
    "\n",
    "#           [[2.8893, 3.6438, 3.6557, 3.6557, 3.6438, 2.8893],\n",
    "#            [3.6438, 4.5361, 4.5508, 4.5508, 4.5361, 3.6438],\n",
    "#            [3.6557, 4.5508, 4.5656, 4.5656, 4.5508, 3.6557],\n",
    "#            [3.6557, 4.5508, 4.5656, 4.5656, 4.5508, 3.6557],\n",
    "#            [3.6438, 4.5361, 4.5508, 4.5508, 4.5361, 3.6438],\n",
    "#            [2.8893, 3.6438, 3.6557, 3.6557, 3.6438, 2.8893]]]]]).cuda()\n",
    "print(target)\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.000001)\n",
    "loss = nn.L1Loss()\n",
    "for epoch in range(100000):\n",
    "    # 清空优化器缓存\n",
    "    opt.zero_grad()\n",
    "    # 前向传播\n",
    "    output = model(input)\n",
    "    if epoch == 99999:\n",
    "        print(model.weight)\n",
    "    # 求loss\n",
    "    c = loss(output, target)\n",
    "    # 反向传播\n",
    "    c.backward()\n",
    "    # 更新参数\n",
    "    opt.step()\n",
    "    if epoch % 500 == 0:\n",
    "        print(\"epoch {:>3d}: loss = {:>8.3f}\".format(epoch, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
